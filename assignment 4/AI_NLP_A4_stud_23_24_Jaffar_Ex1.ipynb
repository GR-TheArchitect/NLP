{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHUHsaYZf29e"
      },
      "source": [
        "# Lab 4: Probing the capabilities of LLMs\n",
        "\n",
        "Unlike previous assignments in this course, our primary goal in this lab is not to use NLP tools and techniques model language *per se*, but rather to investigate properties of language models themselves, since large language models (LLMs) are \"black boxes\" whose inner workings cannot be directly observed.\n",
        "\n",
        "In particular, you will utilize and interpret the outputs of language models to **probe** features of those models--and in particular, how closely (or not) they resemble human language use/knowledge. We will design and implement probes for masked language modeling in BERT, in order to build on our knowledge from Lab 3, but these techniques are very generally applicable to models of all sorts, including generative models like GPT-3.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QdVKyOPkdq2"
      },
      "source": [
        "Masked Language Modeling is essentially a game of \"fill in the blank\". The model is given an input text of which a portion is \"masked\", and trained to predict what the masked element is given the surrounding context (both before and after the mask). Your job is to use these predictions to reason about how the model itself works."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rules\n",
        "* The assignment should submitted to **Blackboard** as `.ipynb`. Only **one submission per group**.\n",
        "\n",
        "* The **filename** should be the group number, e.g., `01.ipynb` or `31.ipynb`.\n",
        "\n",
        "* The questions marked **Extra** or **Optional** are an additional challenge for those interested in going the extra mile. There are no points for them.\n",
        "\n",
        "**Rules for implementation**\n",
        "\n",
        "* You should **write your answers in this iPython Notebook**. (See http://ipython.org/notebook.html for reference material.) If you have problems, please contact your teaching assistant.\n",
        "\n",
        "* Use only **one cell for markdown** answers!    \n",
        "\n",
        "    * You do **not need to submit any code** for this lab, but you are free to leave any code you might run in your submission, so long as it does not interfere with readability of your written responses.\n",
        "    * For text-based questions, put your solution in the `█████ YOUR ANSWER HERE █████` cell and keep the header.\n",
        "\n",
        "* Don't change or delete any initially provided cells, either text or code, unless explicitly instructed to do so.\n",
        "* Don't change the names of provided functions and variables or arguments of the functions.\n",
        "* Leave the output of your code in the output cells.\n",
        "* Test your code and **make sure we can run your notebook** in the colab environment.\n",
        "* Don't forget to fill in the contribution information.\n",
        "\n",
        "<font color=\"red\">You following these rules helps us to grade the submissions relatively efficiently. If these rules are violated, a submission will be subject to penalty points.</font>  "
      ],
      "metadata": {
        "id": "B-HM0RPfrDyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"red\">Contributions</font>\n",
        "\n",
        "~~Delete this text and write instead of it:~~\n",
        "* ~~a list of group members names (NOT student IDs)~~\n",
        "* ~~who contributed to which exercises (you don't need to be very detailed)~~"
      ],
      "metadata": {
        "id": "NGpWAnS_rf-j"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hamhG9qukptp"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMIYAasISpXT"
      },
      "source": [
        "BERT, which you are already familiar with, is pre-trained on a masked language modeling task. We will use this model to make predictions about what \"fills in the blank\" in a masked language task. As before, we will need the transformers package to use BERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eaGaDGcQhwd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e414d07e-143f-44cc-b905-52e87eabe75c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.41.2\n"
          ]
        }
      ],
      "source": [
        "# !pip install transformers\n",
        "import transformers\n",
        "print(transformers.__version__) # 4.41.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTZrGWI3T58X"
      },
      "source": [
        "Then, we need to instantiate the tokenizer and the masked learning model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WpBhMZbKT-vK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228,
          "referenced_widgets": [
            "138ed666907b4b529dd9ae8d3a1ecb33",
            "6ef2e7ed6ce74155b0c10d05a3338687",
            "559a0d9bbe5b41739ed5cedaf549a67c",
            "527057938af044eea43c26519d80e4b8",
            "ef8fe849ccfc45428a904ad9592d43f3"
          ]
        },
        "outputId": "a0ea61f1-2dbf-4808-fa03-f1083383e95f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "138ed666907b4b529dd9ae8d3a1ecb33"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ef2e7ed6ce74155b0c10d05a3338687"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "559a0d9bbe5b41739ed5cedaf549a67c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "527057938af044eea43c26519d80e4b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef8fe849ccfc45428a904ad9592d43f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM, RobertaTokenizer, RobertaModel, RobertaForMaskedLM\n",
        "\n",
        "seed = 5\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "\n",
        "#bert_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkHva4MwbvEO"
      },
      "source": [
        "# Dealing with masked sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3qSWsukVRAX"
      },
      "source": [
        "Next, we want to define some methods to allow us to see the probability of particular candidate tokens to \"fill in the blank\" in some text. We will use the token [MASK] to denote the blank.\n",
        "\n",
        "The class ```MaskedSentence``` takes a sentence with a [MASK] token and uses softmax to turn the weights of all possible predicted values for the mask into probabilities. The class has two additional methods:\n",
        "\n",
        "* The function ```get_masked_token_probability``` takes a string  ```token``` and prints the likelihood that BERT assigns to [MASK] being replaced in the text with ```token```.\n",
        "* The function ```predict_masked_sentence``` prints the top *k* (5 by default) predictions for [MASK] with their probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "n0yOYp_PiUmy"
      },
      "outputs": [],
      "source": [
        "# adapted from code by Yuchen Liu\n",
        "\n",
        "class MaskedSentence:\n",
        "\n",
        "  \"\"\"\n",
        "  A tokenized sentence with a masked word\n",
        "  Note: [MASK] is the default mask token for BERT, other MLMs may have different defaults\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, text, model=bert_model, tokenizer=bert_tokenizer, mask_token=\"[MASK]\"):\n",
        "\n",
        "    # Tokenize text and obtain predictions for mask\n",
        "\n",
        "    self.tokenizer = tokenizer\n",
        "    self.mask = mask_token\n",
        "\n",
        "    text = \"[CLS] %s [SEP]\"%text\n",
        "    tokenized_text = self.tokenizer.tokenize(text)\n",
        "    masked_index = tokenized_text.index(mask_token)\n",
        "    indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensor)\n",
        "        predictions = outputs[0]\n",
        "\n",
        "    # Turn predictions into a probability distribution using softmax\n",
        "\n",
        "    self.probs = torch.nn.functional.softmax(predictions[0, masked_index], dim=-1)\n",
        "\n",
        "  def get_masked_token_probability(self, token):\n",
        "\n",
        "    # prints probability of mask being replaced by token\n",
        "\n",
        "    token_id = self.tokenizer.convert_tokens_to_ids(token)\n",
        "    token_prob = self.probs[token_id]\n",
        "\n",
        "    print(f\"{self.mask}: {token},  | probability:, {float(token_prob)}, \\n\")\n",
        "\n",
        "  def predict_masked_sent(self, top_k=5):\n",
        "\n",
        "    # prints k most probable replacements for token\n",
        "\n",
        "    top_k_weights, top_k_indices = torch.topk(self.probs, top_k, sorted=True)\n",
        "\n",
        "    for i, pred_idx in enumerate(top_k_indices):\n",
        "        predicted_token = self.tokenizer.convert_ids_to_tokens([pred_idx])[0]\n",
        "        token_weight = top_k_weights[i]\n",
        "        print(f\"{self.mask}: {predicted_token}, | probability:, {float(token_weight)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJMBDzCybVBZ"
      },
      "source": [
        "Now, let's test these methods on a string with a mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Zh_yRQdybc1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e10eaa4-fb9a-46f0-8d64-ed4a80fad591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MASK]: stage,  | probability:, 0.0009108720696531236, \n",
            "\n",
            "[MASK]: game, | probability:, 0.22936248779296875\n",
            "[MASK]: team, | probability:, 0.2215639054775238\n",
            "[MASK]: player, | probability:, 0.1818789690732956\n",
            "[MASK]: champion, | probability:, 0.02110450714826584\n",
            "[MASK]: winner, | probability:, 0.012764733284711838\n"
          ]
        }
      ],
      "source": [
        "test_sentence = MaskedSentence(\"All the world’s a [MASK], and all the men and women merely players.\")\n",
        "test_sentence.get_masked_token_probability(\"stage\")\n",
        "test_sentence.predict_masked_sent(top_k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNrjlvxslo1n"
      },
      "source": [
        "We see that the model assigns a probability of about 0.0009 to *stage*, and the highest-probability token is *game* (0.229)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk_WOo8Vm3GJ"
      },
      "source": [
        "# Testing linguistic knowledge in MLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq9DicHynuN7"
      },
      "source": [
        "A major outstanding question in the study of large language models in general is how well (or poorly) the models are able to replicate aspects of humans' implicit linguistic knowledge and linguistic reasoning. One way to test this is to see how the model predicts a target word.\n",
        "\n",
        "For example, consider the ways we could fill in the blank in the sentence *If cats were herbivores, they would probably eat _________.* This sentence is an example of what linguists call a **counterfactual conditional**: a description of what would happen if some hypothetical (but contrary to reality) condition were met. A speaker of English could recognize that this sentence is describing a hypothetical situation in which cats eat only plants, so the most logical continuation would be a word that describes edible plants, like *vegetables* or *carrots*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yxjbZD-sMbF"
      },
      "source": [
        "Let's see what BERT predicts as the likeliest possible predictions for the mask:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "KNcWeescpF3N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f414b4b-13bd-4617-8101-0aca91c94660"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MASK]: them, | probability:, 0.2037343680858612\n",
            "[MASK]: humans, | probability:, 0.19445215165615082\n",
            "[MASK]: it, | probability:, 0.06679246574640274\n",
            "[MASK]: animals, | probability:, 0.028084909543395042\n",
            "[MASK]: meat, | probability:, 0.02714720368385315\n"
          ]
        }
      ],
      "source": [
        "cats_sent = MaskedSentence(\"If cats were herbivores, they would probably eat [MASK].\")\n",
        "cats_sent.predict_masked_sent(top_k=5)\n",
        "\n",
        "#\"If cats were herbivores, they would probably eat [MASK].\"\n",
        "#\"herbivores eat plants. cats are not herbivores. However, if cats were herbivores, they would probably eat [MASK].\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3zsQP_-DK8xm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huems7ZIpHGj"
      },
      "source": [
        "We see that BERT's top 5 predictions for the mask are *them, humans, it*, *animals*, and *meat*. On one hand, all of these predictions result in grammatical (syntactically well-formed) sentences, but they are either not very contentful (*them*, *it*) or nonsensical (*humans*, *meat*, *animals*)---none human-like. By contrast, the probabilities of *vegetables* and *carrots* are both relatively low:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "X2i9q69wQ2OM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e94d1acd-c781-43b5-9c14-4818d00c98b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MASK]: vegetables,  | probability:, 0.0011069991160184145, \n",
            "\n",
            "[MASK]: carrots,  | probability:, 2.656224978636601e-06, \n",
            "\n"
          ]
        }
      ],
      "source": [
        "cats_sent.get_masked_token_probability(\"vegetables\")\n",
        "cats_sent.get_masked_token_probability(\"carrots\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCHgrC7Hsl19"
      },
      "source": [
        "# Ex 1 [1pt] Evaluating counterfactual conditionals in BERT\n",
        "\n",
        "Provide a possible explanation, in 150-200 words, as to why BERT gives such non-human like results for this counterfactual conditional sentence. Your explanation should address the following questions: Do you think this is an arbitrary feature of this one sentence? Or does it reveal something more general about BERT? How could you go about testing whether your explanation is correct, using the class defined above?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">█████ YOUR ANSWER HERE █████</font>\n",
        "\n",
        "According to a recent paper (arXiv:2305.16572), there are a few factors which contribute to this behavior. The paper tests the capacity of pre-trained models including GPT3 and BERT on predicting the consequences of counterfactual conditionals. They find BERT to perform the worst in general and when compared to GPT3. The authors show that models which have a stronger grasp of relevant world knowledge are better equipped to manage counterfactuals. They also find combining relevant world knowledge with simple lexical triggers produce better predictions. The authors further show that, compared to GPT3, a model like BERT largely relies on simple lexical triggers in its predictions of counterfactuals. Based on this paper, we can attribute BERT's poor performance on counterfactuals to a lower grasp of world knowledge and stronger reliance on only structural cues. We can test this by feeding the relevant knowledge to BERT and then prompting it to predict the most likely words. For example, if we modify the sentence above to \"Herbivores eat plants. cats are not herbivores. However, if cats were herbivores, they would probably eat [MASK].\", the likelihood of the masked word being \"plants\" increases to 61%.   "
      ],
      "metadata": {
        "id": "BOepmgwStHI0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lddRei19VrMz"
      },
      "source": [
        "# Ex2 [4pt] Design your own BERT probe experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQG1STzfeM4M"
      },
      "source": [
        "We can reason about the capabilities of an LLM simply by choosing carefully designed inputs and evaluating the model's corresponding outputs. If we have test many inputs with some shared property, such as a particular syntactic structure, we can start to generalize about BERT's behaviour with text that has that property. For example, we can investigate the question of whether or not BERT predicts continuations of counterfactual conditionals which are consistent with the hypothetical scenario presented in the *if*-clause of the conditional by evaluating what happens when we give it many such conditionals as inputs.\n",
        "\n",
        "Your primary task for this final lab is to design a small experiment that tests, using the same kinds of techniques as above, the capabilities of BERT in a particular domain of your choice. To give you some ideas, here are a few suggestions of possible general domains that could be worth investigating, although the actual question you investigate should be small enough that it can be tested with a relatively modest selection of sentences. You are also free to come up with your own idea:\n",
        "\n",
        "* The interpretation of pronouns (can BERT recognize which individual a pronoun like *it* is referring to when there are multiple possible options?)\n",
        "* Does BERT fall for so-called \"semantic illusions\", in which it fails to recognize an inaccuracy in text, such as answering the question \"How many of each animal did Moses take on the ark?\" with \"2\"? (The Biblical story is about Noah, not Moses.)\n",
        "* Bias: Does BERT make predictions which are more consistent with gender, racial, or other stereotypes?\n",
        "* World knowledge: Does BERT make predictions which correspond with the way the world actually is?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mvDWwhTfc5D"
      },
      "source": [
        "Your description of your experiment should have the following parts and be approximately equivalent in length to 1 typed page (roughly 500 words, in addition to your test sentences):\n",
        "\n",
        "\n",
        "\n",
        "*   **Research Question**: A clear formulation of the question you intend to investigate. It should be small and precise enough that it can reasonably be investigated using the functions defined above.\n",
        "*   **Hypothesis**: The answer to the research question that you predict to be true, and *why you have that specific expectation*.\n",
        "*   **At least 10 test sentences**, with a description of which of their properties are relevant. Be very clear about what, specifically, you are testing, and how the results will bear on your hypothesis.\n",
        "*   **Test** your sentences and see what outputs you get. Do these provide evidence for or against your hypothesis? Why do you think you got the results you did?  \n",
        "\n",
        "*   **Discuss** whether, given your own linguistic intuitions, the behaviour of BERT approximates that of a human language user with respect to your research question. If it is not human-like, how could the model be improved (in terms of training data, architecture, etc.) to achieve better results?\n",
        "* **OPTIONAL**: Try investigating your research question in some other models (see https://huggingface.co/models for some options). You will likely need to adapt your probes for other kinds of models--for instance, a probe you test in a dialogue-based interface like ChatGPT will be different than those you designed for BERT. Some models, like [RoBERTa](https://huggingface.co/FacebookAI/roberta-base), are built upon the BERT architecture, so they are compatible with the code above (see example below--note that RoBERTa tokenizes slightly differently from BERT)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "roberta_model = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
        "\n",
        "#Note 1: RoBERTa's default mask token is <mask>\n",
        "#Note 2: Most tokens in RoBERTa begin with the unusual character Ġ.\n",
        "#This is an artefact of the tokenization process, which includes the space preceding words.\n",
        "#To get the probability of a token \"word\",\n",
        "#we need to give get_masked_token_probability \"Ġword\"\n",
        "\n",
        "roberta_test_sentence = MaskedSentence(\"All the world’s a <mask>, and all the men and women merely players.\", model=roberta_model, tokenizer=roberta_tokenizer, mask_token='<mask>')\n",
        "roberta_test_sentence.get_masked_token_probability(\"Ġstage\")\n",
        "roberta_test_sentence.predict_masked_sent(top_k=5)"
      ],
      "metadata": {
        "id": "6u0UmEwIj4nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">█████ YOUR ANSWER HERE █████</font>"
      ],
      "metadata": {
        "id": "hcehahWutEdk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upqe9EP2tBD8"
      },
      "source": [
        "# Acknowledgments\n",
        "\n",
        "Concept and lab designed by Tom Roberts. BERT MLM script was heavily based on work by Yuchen Lin. Counterfactual example adapted from [Li, Yu, and Ettinger (2023)](https://aclanthology.org/2023.acl-short.70.pdf)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "138ed666907b4b529dd9ae8d3a1ecb33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7304e60c88541c4a2a5ae6393801cd0",
              "IPY_MODEL_5e264bf76b5d4f88b337a63a662737a3",
              "IPY_MODEL_5516e943de834e4cafe132d8a873a754"
            ],
            "layout": "IPY_MODEL_c33c6364c1764cb7bc28405789313f50"
          }
        },
        "6ef2e7ed6ce74155b0c10d05a3338687": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0378630852ce418e942c32a41facc11b",
              "IPY_MODEL_6a4a43d82b5b4fdf8033be9ac2469c7b",
              "IPY_MODEL_97754539daef4e849ab12ce1113862d9"
            ],
            "layout": "IPY_MODEL_d8181cc3277145eb8aad3d7f39957beb"
          }
        },
        "559a0d9bbe5b41739ed5cedaf549a67c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e5f53648344f4fc8aea45b2901309ccb",
              "IPY_MODEL_71276bc8c29f4535be5f5575c07dfe32",
              "IPY_MODEL_38bc443709654d3eb4cfefa30b3ed2c7"
            ],
            "layout": "IPY_MODEL_7799d369aee94cd0a88fbb953e2cd4d6"
          }
        },
        "527057938af044eea43c26519d80e4b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3eb6465a67944b7a4b676784ed64ee8",
              "IPY_MODEL_e4fe1930217e41d097b5a76ab5197a41",
              "IPY_MODEL_9e374ea90fe043e1bf72f794d580f2a5"
            ],
            "layout": "IPY_MODEL_4b69d242ddcf4c4fba281bbbd6256795"
          }
        },
        "ef8fe849ccfc45428a904ad9592d43f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5fd1ffd2ceed44f5bdd42aec22e7f871",
              "IPY_MODEL_682d6d977cd144b783bdbeb7d996f018",
              "IPY_MODEL_9cdddf041f0645eebc63f7a073b3d83d"
            ],
            "layout": "IPY_MODEL_8af0aac8bd9f409b8134e204a05aa284"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}